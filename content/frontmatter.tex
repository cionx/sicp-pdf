\chapter*{Unofficial Texinfo Format}
\addcontentsline{toc}{chapter}{Unofficial Texinfo Format}
\label{UTF}

This is the second edition \acronym{SICP} book, from Unofficial Texinfo
Format.

You are probably reading it in an Info hypertext browser, such as the Info
mode of Emacs.  You might alternatively be reading it {\TeX}-formatted on your
screen or printer, though that would be silly.  And, if printed, expensive.

The freely-distributed official \acronym{HTML}-and-\acronym{GIF} format was
first converted personally to Unofficial Texinfo Format (\acronym{UTF})
version 1 by Lytha Ayth during a long Emacs lovefest weekend in April, 2001.

The \acronym{UTF} is easier to search than the \acronym{HTML} format.  It is
also much more accessible to people running on modest computers, such as
donated '386-based PCs.  A 386 can, in theory, run Linux, Emacs, and a Scheme
interpreter simultaneously, but most 386s probably can't also run both Netscape
and the necessary X Window System without prematurely introducing budding young
underfunded hackers to the concept of \newterm{thrashing}.  \acronym{UTF} can also fit
uncompressed on a 1.44\acronym{MB} floppy diskette, which may come in handy for
installing \acronym{UTF} on PCs that do not have Internet or \acronym{LAN} access.

The Texinfo conversion has been a straight transliteration, to the extent
possible.  Like the {\TeX}-to-\acronym{HTML} conversion, this was not without
some introduction of breakage.  In the case of Unofficial Texinfo Format,
figures have suffered an amateurish resurrection of the lost art of
\acronym{ASCII}.  Also, it's quite possible that some errors of ambiguity
were introduced during the conversion of some of the copious superscripts (`\^{}')
and subscripts (`\_').  Divining \emph{which} has been left as an exercise to
the reader. But at least we don't put our brave astronauts at risk by encoding
the \emph{greater-than-or-equal} symbol as \code{<u>\&gt;</u>}.

If you modify \texttt{sicp.texi} to correct errors or improve the
\acronym{ASCII} art, then update the \code{@set utfversion 2.andresraba5.6}
line to reflect your delta.  For example, if you started with Lytha's version
\code{1}, and your name is Bob, then you could name your successive versions
\code{1.bob1}, \code{1.bob2}, \( \dots \) \code{1.bob\textit{n}}.  Also update
\code{utfversiondate}.  If you want to distribute your version on the Web, then
embedding the string ``sicp.texi'' somewhere in the file or Web page will make
it easier for people to find with Web search engines.

It is believed that the Unofficial Texinfo Format is in keeping with the
spirit of the graciously freely-distributed \acronym{HTML} version.  But you
never know when someone's armada of lawyers might need something to do, and get
their shorts all in a knot over some benign little thing, so think twice before
you use your full name or distribute Info, \acronym{DVI}, PostScript, or
\acronym{PDF} formats that might embed your account or machine name.

\vspace{0.5em}
\noindent
\textit{Peath, Lytha Ayth}

\vspace{1.0em}
\noindent
\textbf{Addendum:} See also the \acronym{SICP} video lectures by Abelson and Sussman:\\
at \href{http://groups.csail.mit.edu/mac/classes/6.001/abelson-sussman-lectures/}{\acronym{MIT CSAIL}} or
\href{http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-001-structure-and-interpretation-of-computer-programs-spring-2005/video-lectures/}{\acronym{MIT OCW}}.

\vspace{0.5em}
\noindent
\textbf{Second Addendum:} Above is the original introduction to the \acronym{UTF}
from 2001. Ten years later, \acronym{UTF} has been transformed: mathematical
symbols and formulas are properly typeset, and figures drawn in
vector graphics. The original text formulas and \acronym{ASCII} art figures
are still there in the Texinfo source, but will display only when compiled
to Info output. At the dawn of e-book readers and tablets, reading a
\acronym{PDF} on screen is officially not silly anymore. Enjoy!

\enlargethispage{\baselineskip}

\vspace{0.5em}
\noindent
\textit{A.R, May, 2011}

\chapter*{Dedication}
\addcontentsline{toc}{chapter}{Dedication}
\label{Dedication}

\lettrine{T}{his book is dedicated}, in respect and admiration, to the spirit that lives in
the computer.

\begin{quote}
``I think that it's extraordinarily important that we in computer science keep
fun in computing.  When it started out, it was an awful lot of fun.  Of course,
the paying customers got shafted every now and then, and after a while we began
to take their complaints seriously.  We began to feel as if we really were
responsible for the successful, error-free perfect use of these machines.  I
don't think we are.  I think we're responsible for stretching them, setting
them off in new directions, and keeping fun in the house.  I hope the field of
computer science never loses its sense of fun.  Above all, I hope we don't
become missionaries.  Don't feel as if you're Bible salesmen.  The world has
too many of those already.  What you know about computing other people will
learn.  Don't feel as if the key to successful computing is only in your hands.
What's in your hands, I think and hope, is intelligence: the ability to see the
machine as more than when you were first led up to it, that you can make it
more.''

\enlargethispage{\baselineskip}

\noindent
---Alan J. Perlis (April 1, 1922 -- February 7, 1990)
\end{quote}

\chapter*{Foreword}
\addcontentsline{toc}{chapter}{Foreword}
\label{Foreword}

% \vspace{-0.6em}

\lettrine{E}{ducators, generals, dieticians}, psychologists, and parents program.  Ar\-mies,
students, and some societies are programmed.  An assault on large problems
employs a succession of programs, most of which spring into existence en route.
These programs are rife with issues that appear to be particular to the problem
at hand.  To appreciate programming as an intellectual activity in its own
right you must turn to computer programming; you must read and write computer
programs---many of them.  It doesn't matter much what the programs are about or
what applications they serve.  What does matter is how well they perform and
how smoothly they fit with other programs in the creation of still greater
programs.  The programmer must seek both perfection of part and adequacy of
collection.  In this book the use of ``program'' is focused on the creation,
execution, and study of programs written in a dialect of Lisp for execution on
a digital computer.  Using Lisp we restrict or limit not what we may program,
but only the notation for our program descriptions.

Our traffic with the subject matter of this book involves us with three foci of
phenomena: the human mind, collections of computer programs, and the computer.
Every computer program is a model, hatched in the mind, of a real or mental
process.  These processes, arising from human experience and thought, are huge
in number, intricate in detail, and at any time only partially understood.
They are modeled to our permanent satisfaction rarely by our computer programs.
Thus even though our programs are carefully handcrafted discrete collections of
symbols, mosaics of interlocking functions, they continually evolve: we change
them as our perception of the model deepens, enlarges, generalizes until the
model ultimately attains a metastable place within still another model with
which we struggle.  The source of the exhilaration associated with computer
programming is the continual unfolding within the mind and on the computer of
mechanisms expressed as programs and the explosion of perception they generate.
If art interprets our dreams, the computer executes them in the guise of
programs!

For all its power, the computer is a harsh taskmaster.  Its programs must be
correct, and what we wish to say must be said accurately in every detail.  As
in every other symbolic activity, we become convinced of program truth through
argument.  Lisp itself can be assigned a semantics (another model, by the way),
and if a program's function can be specified, say, in the predicate calculus,
the proof methods of logic can be used to make an acceptable correctness
argument.  Unfortunately, as programs get large and complicated, as they almost
always do, the adequacy, consistency, and correctness of the specifications
themselves become open to doubt, so that complete formal arguments of
correctness seldom accompany large programs.  Since large programs grow from
small ones, it is crucial that we develop an arsenal of standard program
structures of whose correctness we have become sure---we call them idioms---and
learn to combine them into larger structures using organizational techniques of
proven value.  These techniques are treated at length in this book, and
understanding them is essential to participation in the Promethean enterprise
called programming.  More than anything else, the uncovering and mastery of
powerful organizational techniques accelerates our ability to create large,
significant programs.  Conversely, since writing large programs is very taxing,
we are stimulated to invent new methods of reducing the mass of function and
detail to be fitted into large programs.

Unlike programs, computers must obey the laws of physics.  If they wish to
perform rapidly---a few nanoseconds per state change---they must transmit
electrons only small distances (at most \( 1 {1\over2} \) feet). The heat generated by
the huge number of devices so concentrated in space has to be removed.  An
exquisite engineering art has been developed balancing between multiplicity of
function and density of devices.  In any event, hardware always operates at a
level more primitive than that at which we care to program.  The processes that
transform our Lisp programs to ``machine'' programs are themselves abstract
models which we program.  Their study and creation give a great deal of insight
into the organizational programs associated with programming arbitrary models.
Of course the computer itself can be so modeled.  Think of it: the behavior of
the smallest physical switching element is modeled by quantum mechanics
described by differential equations whose detailed behavior is captured by
numerical approximations represented in computer programs executing on
computers composed of \( \dots \)!

It is not merely a matter of tactical convenience to separately identify the
three foci.  Even though, as they say, it's all in the head, this logical
separation induces an acceleration of symbolic traffic between these foci whose
richness, vitality, and potential is exceeded in human experience only by the
evolution of life itself.  At best, relationships between the foci are
metastable.  The computers are never large enough or fast enough.  Each
breakthrough in hardware technology leads to more massive programming
enterprises, new organizational principles, and an enrichment of abstract
models.  Every reader should ask himself periodically ``Toward what end, toward
what end?''---but do not ask it too often lest you pass up the fun of
programming for the constipation of bittersweet philosophy.

Among the programs we write, some (but never enough) perform a precise
mathematical function such as sorting or finding the maximum of a sequence of
numbers, determining primality, or finding the square root.  We call such
programs algorithms, and a great deal is known of their optimal behavior,
particularly with respect to the two important parameters of execution time and
data storage requirements.  A programmer should acquire good algorithms and
idioms.  Even though some programs resist precise specifications, it is the
responsibility of the programmer to estimate, and always to attempt to improve,
their performance.

Lisp is a survivor, having been in use for about a quarter of a century.  Among
the active programming languages only Fortran has had a longer life.  Both
languages have supported the programming needs of important areas of
application, Fortran for scientific and engineering computation and Lisp for
artificial intelligence.  These two areas continue to be important, and their
programmers are so devoted to these two languages that Lisp and Fortran may
well continue in active use for at least another quarter-century.

Lisp changes.  The Scheme dialect used in this text has evolved from the
original Lisp and differs from the latter in several important ways, including
static scoping for variable binding and permitting functions to yield functions
as values.  In its semantic structure Scheme is as closely akin to Algol 60 as
to early Lisps.  Algol 60, never to be an active language again, lives on in
the genes of Scheme and Pascal.  It would be difficult to find two languages
that are the communicating coin of two more different cultures than those
gathered around these two languages.  Pascal is for building
pyramids---imposing, breathtaking, static structures built by armies pushing
heavy blocks into place.  Lisp is for building organisms---imposing,
breathtaking, dynamic structures built by squads fitting fluctuating myriads of
simpler organisms into place.  The organizing principles used are the same in
both cases, except for one extraordinarily important difference: The
discretionary exportable functionality entrusted to the individual Lisp
programmer is more than an order of magnitude greater than that to be found
within Pascal enterprises.  Lisp programs inflate libraries with functions
whose utility transcends the application that produced them.  The list, Lisp's
native data structure, is largely responsible for such growth of utility.  The
simple structure and natural applicability of lists are reflected in functions
that are amazingly nonidiosyncratic.  In Pascal the plethora of declarable data
structures induces a specialization within functions that inhibits and
penalizes casual cooperation.  It is better to have 100 functions operate on
one data structure than to have 10 functions operate on 10 data structures.  As
a result the pyramid must stand unchanged for a millennium; the organism must
evolve or perish.

To illustrate this difference, compare the treatment of material and exercises
within this book with that in any first-course text using Pascal.  Do not labor
under the illusion that this is a text digestible at \acronym{MIT} only,
peculiar to the breed found there.  It is precisely what a serious book on
programming Lisp must be, no matter who the student is or where it is used.

Note that this is a text about programming, unlike most Lisp books, which are
used as a preparation for work in artificial intelligence.  After all, the
critical programming concerns of software engineering and artificial
intelligence tend to coalesce as the systems under investigation become larger.
This explains why there is such growing interest in Lisp outside of artificial
intelligence.

As one would expect from its goals, artificial intelligence research generates
many significant programming problems.  In other programming cultures this
spate of problems spawns new languages.  Indeed, in any very large programming
task a useful organizing principle is to control and isolate traffic within the
task modules via the invention of language.  These languages tend to become
less primitive as one approaches the boundaries of the system where we humans
interact most often.  As a result, such systems contain complex
language-processing functions replicated many times.  Lisp has such a simple
syntax and semantics that parsing can be treated as an elementary task.  Thus
parsing technology plays almost no role in Lisp programs, and the construction
of language processors is rarely an impediment to the rate of growth and change
of large Lisp systems.  Finally, it is this very simplicity of syntax and
semantics that is responsible for the burden and freedom borne by all Lisp
programmers.  No Lisp program of any size beyond a few lines can be written
without being saturated with discretionary functions.  Invent and fit; have
fits and reinvent!  We toast the Lisp programmer who pens his thoughts within
nests of parentheses.

\vspace{0.5em}
\noindent
Alan J. Perlis\\
New Haven, Connecticut

\chapter*{Preface to the Second Edition}
\addcontentsline{toc}{chapter}{Preface to the Second Edition}
\label{Preface}

\begin{quote}
Is it possible that software is not like anything else, that it is meant to be
discarded: that the whole point is to always see it as a soap bubble?

---Alan J. Perlis
\end{quote}

% \vspace{0.7em}

\noindent
\lettrine{T}{he material in this book} has been the basis of \acronym{MIT}'s entry-level
computer science subject since 1980.  We had been teaching this material for
four years when the first edition was published, and twelve more years have
elapsed until the appearance of this second edition.  We are pleased that our
work has been widely adopted and incorporated into other texts.  We have seen
our students take the ideas and programs in this book and build them in as the
core of new computer systems and languages.  In literal realization of an
ancient Talmudic pun, our students have become our builders.  We are lucky to
have such capable students and such accomplished builders.

In preparing this edition, we have incorporated hundreds of clarifications
suggested by our own teaching experience and the comments of colleagues at
\acronym{MIT} and elsewhere.  We have redesigned most of the major programming
systems in the book, including the generic-arithmetic system, the interpreters,
the register-machine simulator, and the compiler; and we have rewritten all the
program examples to ensure that any Scheme implementation conforming to the
\acronym{IEEE} Scheme standard (\link{IEEE 1990}) will be able to run the
code.

This edition emphasizes several new themes.  The most important of these is the
central role played by different approaches to dealing with time in
computational models: objects with state, concurrent programming, functional
programming, lazy evaluation, and nondeterministic programming.  We have
included new sections on concurrency and nondeterminism, and we have tried to
integrate this theme throughout the book.

The first edition of the book closely followed the syllabus of our
\acronym{MIT} one-semester subject.  With all the new material in the second
edition, it will not be possible to cover everything in a single semester, so
the instructor will have to pick and choose.  In our own teaching, we sometimes
skip the section on logic programming (\link{Section 4.4}), we have students use
the register-machine simulator but we do not cover its implementation (\link{Section 5.2}),
and we give only a cursory overview of the compiler (\link{Section 5.5}).
Even so, this is still an intense course.  Some instructors may
wish to cover only the first three or four chapters, leaving the other material
for subsequent courses.

The World-Wide-Web site \href{http://mitpress.mit.edu/sicp}{http://mitpress.mit.edu/sicp} provides
support for users of this book.  This includes programs from the book, sample
programming assignments, supplementary materials, and downloadable
implementations of the Scheme dialect of Lisp.

\chapter*{Preface to the First Edition}
\addcontentsline{toc}{chapter}{Preface to the First Edition}
\label{Preface 1e}

% \vspace{-0.6em}
\begin{quote}
A computer is like a violin.  You can imagine a novice trying first a
phonograph and then a violin.  The latter, he says, sounds terrible.  That is
the argument we have heard from our humanists and most of our computer
scientists.  Computer programs are good, they say, for particular purposes, but
they aren't flexible.  Neither is a violin, or a typewriter, until you learn
how to use it.

---Marvin Minsky, ``Why Programming Is a Good Medium for Expressing
Poorly-Understood and Sloppily-Formulated Ideas''
\end{quote}

% \vspace{0.8em}

\noindent
\lettrine[lhang=0.17]{``T}{he Structure and Interpretation of Computer Programs''} is the entry-level
subject in computer science at the Massachusetts Institute of Technology.  It
is required of all students at \acronym{MIT} who major in electrical
engineering or in computer science, as one-fourth of the ``common core
curriculum,'' which also includes two subjects on circuits and linear systems
and a subject on the design of digital systems.  We have been involved in the
development of this subject since 1978, and we have taught this material in its
present form since the fall of 1980 to between 600 and 700 students each year.
Most of these students have had little or no prior formal training in
computation, although many have played with computers a bit and a few have had
extensive programming or hardware-design experience.

Our design of this introductory computer-science subject reflects two major
concerns.  First, we want to establish the idea that a computer language is not
just a way of getting a computer to perform operations but rather that it is a
novel formal medium for expressing ideas about methodology.  Thus, programs
must be written for people to read, and only incidentally for machines to
execute.  Second, we believe that the essential material to be addressed by a
subject at this level is not the syntax of particular programming-language
constructs, nor clever algorithms for computing particular functions
efficiently, nor even the mathematical analysis of algorithms and the
foundations of computing, but rather the techniques used to control the
intellectual complexity of large software systems.

Our goal is that students who complete this subject should have a good feel for
the elements of style and the aesthetics of programming.  They should have
command of the major techniques for controlling complexity in a large
system. They should be capable of reading a 50-page-long program, if it is
written in an exemplary style. They should know what not to read, and what they
need not understand at any moment.  They should feel secure about modifying a
program, retaining the spirit and style of the original author.

These skills are by no means unique to computer programming.  The techniques we
teach and draw upon are common to all of engineering design.  We control
complexity by building abstractions that hide details when appropriate.  We
control complexity by establishing conventional interfaces that enable us to
construct systems by combining standard, well-understood pieces in a ``mix and
match'' way.  We control complexity by establishing new languages for
describing a design, each of which emphasizes particular aspects of the design
and deemphasizes others.

Underlying our approach to this subject is our conviction that ``computer
science'' is not a science and that its significance has little to do with
computers.  The computer revolution is a revolution in the way we think and in
the way we express what we think.  The essence of this change is the emergence
of what might best be called \newterm{procedural epistemology}---the study of
the structure of knowledge from an imperative point of view, as opposed to the
more declarative point of view taken by classical mathematical subjects.
Mathematics provides a framework for dealing precisely with notions of ``what
is.''  Computation provides a framework for dealing precisely with notions of
``how to.''

In teaching our material we use a dialect of the programming language Lisp.  We
never formally teach the language, because we don't have to.  We just use it,
and students pick it up in a few days.  This is one great advantage of
Lisp-like languages: They have very few ways of forming compound expressions,
and almost no syntactic structure.  All of the formal properties can be covered
in an hour, like the rules of chess.  After a short time we forget about
syntactic details of the language (because there are none) and get on with the
real issues---figuring out what we want to compute, how we will decompose
problems into manageable parts, and how we will work on the parts.  Another
advantage of Lisp is that it supports (but does not enforce) more of the
large-scale strategies for modular decomposition of programs than any other
language we know.  We can make procedural and data abstractions, we can use
higher-order functions to capture common patterns of usage, we can model local
state using assignment and data mutation, we can link parts of a program with
streams and delayed evaluation, and we can easily implement embedded languages.
All of this is embedded in an interactive environment with excellent support
for incremental program design, construction, testing, and debugging.  We thank
all the generations of Lisp wizards, starting with John McCarthy, who have
fashioned a fine tool of unprecedented power and elegance.

Scheme, the dialect of Lisp that we use, is an attempt to bring together the
power and elegance of Lisp and Algol.  From Lisp we take the metalinguistic
power that derives from the simple syntax, the uniform representation of
programs as data objects, and the garbage-collected heap-allocated data.  From
Algol we take lexical scoping and block structure, which are gifts from the
pioneers of programming-language design who were on the Algol committee.  We
wish to cite John Reynolds and Peter Landin for their insights into the
relationship of Church's λ-calculus to the structure of programming
languages.  We also recognize our debt to the mathematicians who scouted out
this territory decades before computers appeared on the scene.  These pioneers
include Alonzo Church, Barkley Rosser, Stephen Kleene, and Haskell Curry.

\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}
\label{Acknowledgments}

\lettrine[findent=1pt]{W}{e would like to thank} the many people who have helped us develop this book and
this curriculum.

Our subject is a clear intellectual descendant of ``6.231,'' a wonderful
subject on programming linguistics and the λ-calculus taught at
\acronym{MIT} in the late 1960s by Jack Wozencraft and Arthur Evans, Jr.

We owe a great debt to Robert Fano, who reorganized \acronym{MIT}'s
introductory curriculum in electrical engineering and computer science to
emphasize the principles of engineering design.  He led us in starting out on
this enterprise and wrote the first set of subject notes from which this book
evolved.

Much of the style and aesthetics of programming that we try to teach were
developed in conjunction with Guy Lewis Steele Jr., who collaborated with
Gerald Jay Sussman in the initial development of the Scheme language.  In
addition, David Turner, Peter Henderson, Dan Friedman, David Wise, and Will
Clinger have taught us many of the techniques of the functional programming
community that appear in this book.

Joel Moses taught us about structuring large systems.  His experience with the
Macsyma system for symbolic computation provided the insight that one should
avoid complexities of control and concentrate on organizing the data to reflect
the real structure of the world being modeled.

Marvin Minsky and Seymour Papert formed many of our attitudes about programming
and its place in our intellectual lives.  To them we owe the understanding that
computation provides a means of expression for exploring ideas that would
otherwise be too complex to deal with precisely.  They emphasize that a
student's ability to write and modify programs provides a powerful medium in
which exploring becomes a natural activity.

We also strongly agree with Alan Perlis that programming is lots of fun and we
had better be careful to support the joy of programming.  Part of this joy
derives from observing great masters at work.  We are fortunate to have been
apprentice programmers at the feet of Bill Gosper and Richard Greenblatt.

It is difficult to identify all the people who have contributed to the
development of our curriculum.  We thank all the lecturers, recitation
instructors, and tutors who have worked with us over the past fifteen years and
put in many extra hours on our subject, especially Bill Siebert, Albert Meyer,
Joe Stoy, Randy Davis, Louis Braida, Eric Grimson, Rod Brooks, Lynn Stein and
Peter Szolovits.  We would like to specially acknowledge the outstanding
teaching contributions of Franklyn Turbak, now at Wellesley; his work in
undergraduate instruction set a standard that we can all aspire to.  We are
grateful to Jerry Saltzer and Jim Miller for helping us grapple with the
mysteries of concurrency, and to Peter Szolovits and David McAllester for their
contributions to the exposition of nondeterministic evaluation in \link{Chapter
4}.

Many people have put in significant effort presenting this material at other
universities.  Some of the people we have worked closely with are Jacob
Katzenelson at the Technion, Hardy Mayer at the University of California at
Irvine, Joe Stoy at Oxford, Elisha Sacks at Purdue, and Jan Komorowski at the
Norwegian University of Science and Technology.  We are exceptionally proud of
our colleagues who have received major teaching awards for their adaptations of
this subject at other universities, including Kenneth Yip at Yale, Brian Harvey
at the University of California at Berkeley, and Dan Huttenlocher at Cornell.

Al Moy\'e arranged for us to teach this material to engineers at
Hewlett-Packard, and for the production of videotapes of these lectures.  We
would like to thank the talented instructors---in particular Jim Miller, Bill
Siebert, and Mike Eisenberg---who have designed continuing education courses
incorporating these tapes and taught them at universities and industry all over
the world.

Many educators in other countries have put in significant work translating the
first edition.  Michel Briand, Pierre Chamard, and Andr\'e Pic produced a
French edition; Susanne Daniels-Herold produced a German edition; and Fumio
Motoyoshi produced a Japanese edition.  We do not know who produced the Chinese
edition, but we consider it an honor to have been selected as the subject of an
``unauthorized'' translation.

It is hard to enumerate all the people who have made technical contributions to
the development of the Scheme systems we use for instructional purposes.  In
addition to Guy Steele, principal wizards have included Chris Hanson, Joe
Bowbeer, Jim Miller, Guillermo Rozas, and Stephen Adams.  Others who have put
in significant time are Richard Stallman, Alan Bawden, Kent Pitman, Jon Taft,
Neil Mayle, John Lamping, Gwyn Osnos, Tracy Larrabee, George Carrette, Soma
Chaudhuri, Bill Chiarchiaro, Steven Kirsch, Leigh Klotz, Wayne Noss, Todd Cass,
Patrick O'Donnell, Kevin Theobald, Daniel Weise, Kenneth Sinclair, Anthony
Courtemanche, Henry M. Wu, Andrew Berlin, and Ruth Shyu.

Beyond the \acronym{MIT} implementation, we would like to thank the many people
who worked on the \acronym{IEEE} Scheme standard, including William Clinger and
Jonathan Rees, who edited the \( \rm R^4RS \), and Chris Haynes, David Bartley,
Chris Hanson, and Jim Miller, who prepared the \acronym{IEEE} standard.

Dan Friedman has been a long-time leader of the Scheme community.  The
community's broader work goes beyond issues of language design to encompass
significant educational innovations, such as the high-school curriculum based
on EdScheme by Schemer's Inc., and the wonderful books by Mike Eisenberg and by
Brian Harvey and Matthew Wright.

We appreciate the work of those who contributed to making this a real book,
especially Terry Ehling, Larry Cohen, and Paul Bethge at the \acronym{MIT}
Press.  Ella Mazel found the wonderful cover image.  For the second edition we
are particularly grateful to Bernard and Ella Mazel for help with the book
design, and to David Jones, {\TeX} wizard extraordinaire.  We also are indebted
to those readers who made penetrating comments on the new draft: Jacob
Katzenelson, Hardy Mayer, Jim Miller, and especially Brian Harvey, who did unto
this book as Julie did unto his book \textit{Simply Scheme}.

Finally, we would like to acknowledge the support of the organizations that
have encouraged this work over the years, including support from
Hewlett-Packard, made possible by Ira Goldstein and Joel Birnbaum, and support
from \acronym{DARPA}, made possible by Bob Kahn.
