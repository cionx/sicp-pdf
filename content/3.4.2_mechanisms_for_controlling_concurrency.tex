\subsection{Mechanisms for Controlling Concurrency}
\label{Section 3.4.2}

We’ve seen that the difficulty in dealing with concurrent processes is rooted in the need to consider the interleaving of the order of events in the different processes.
For example, suppose we have two processes, one with three ordered events \( (a, b, c) \) and one with three ordered events \( (x, y, z) \).
If the two processes run concurrently, with no constraints on how their execution is interleaved, then there are 20 different possible orderings for the events that are consistent with the individual orderings for the two processes:
\begin{example}
	(a,b,c,x,y,z)  (a,x,b,y,c,z)  (x,a,b,c,y,z)  (x,a,y,z,b,c)
	(a,b,x,c,y,z)  (a,x,b,y,z,c)  (x,a,b,y,c,z)  (x,y,a,b,c,z)
	(a,b,x,y,c,z)  (a,x,y,b,c,z)  (x,a,b,y,z,c)  (x,y,a,b,z,c)
	(a,b,x,y,z,c)  (a,x,y,b,z,c)  (x,a,y,b,c,z)  (x,y,a,z,b,c)
	(a,x,b,c,y,z)  (a,x,y,z,b,c)  (x,a,y,b,z,c)  (x,y,z,a,b,c)
\end{example}
As programmers designing this system, we would have to consider the effects of each of these 20 orderings and check that each behavior is acceptable.
Such an approach rapidly becomes unwieldy as the numbers of processes and events increase.

A more practical approach to the design of concurrent systems is to devise general mechanisms that allow us to constrain the interleaving of concurrent processes so that we can be sure that the program behavior is correct.
Many mechanisms have been developed for this purpose.
In this section, we describe one of them, the \newterm{serializer}.



\subsubsection*{Serializing access to shared state}

Serialization implements the following idea:
Processes will execute concurrently, but there will be certain collections of procedures that cannot be executed concurrently.
More precisely, serialization creates distinguished sets of procedures such that only one execution of a procedure in each serialized set is permitted to happen at a time.
If some procedure in the set is being executed, then a process that attempts to execute any procedure in the set will be forced to wait until the first execution has finished.

We can use serialization to control access to shared variables.
For example, if we want to update a shared variable based on the previous value of that variable, we put the access to the previous value of the variable and the assignment of the new value to the variable in the same procedure.
We then ensure that no other procedure that assigns to the variable can run concurrently with this procedure by serializing all of these procedures with the same serializer.
This guarantees that the value of the variable cannot be changed between an access and the corresponding assignment.



\subsubsection*{Serializers in Scheme}

To make the above mechanism more concrete, suppose that we have extended Scheme to include a procedure called \code{parallel-execute}:

\begin{scheme}
  (parallel-execute) ⟨\var{p}\ind{1}⟩ ⟨\var{p}\ind{1}⟩ … ⟨\var{p}\ind{k}⟩
\end{scheme}

\noindent
Each \code{⟨p⟩} must be a procedure of no arguments.
\code{parallel-execute} creates a separate process for each \code{⟨p⟩}, which applies \code{⟨p⟩} (to no arguments).
These processes all run concurrently.%
\footnote{
	\code{parallel-execute} is not part of standard Scheme, but it can be implemented in \acronym{MIT} Scheme.
	In our implementation, the new concurrent processes also run concurrently with the original Scheme process.
	Also, in our implementation, the value returned by \code{parallel-execute} is a special control object that can be used to halt the newly created processes.
}

As an example of how this is used, consider
\begin{scheme}
  (define x 10)
  (parallel-execute
   (lambda () (set! x (* x x)))
   (lambda () (set! x (+ x 1))))
\end{scheme}
This creates two concurrent processes---\( P_1 \), which sets \code{x} to \code{x} times \code{x}, and \( P_2 \), which increments \code{x}.
After execution is complete, \code{x} will be left with one of five possible values, depending on the interleaving of the events of \( P_1 \) and \( P_2 \):

\begin{center}
	\begin{tabular}{rl}
		\toprule
		\code{101}: &
		\( P_1 \) sets \code{x} to \( 100 \) and then \( P_2 \) increments \code{x} to \( 101 \).
		\\
		\code{121}: &
		\( P_2 \) increments \code{x} to \( 11 \) and then \( P_1 \) sets \code{x} to \code{x} times \code{x}.
		\\
		\code{110}: &
		\( P_2 \) changes \code{x} from \( 10 \) to \( 11 \) between the two times that \\
		        {}  &
		\( P_1 \) accesses the value of \code{x} during the evaluation of \code{(* x x)}.
		\\
		\code{11}: &
		\( P_2 \) accesses \code{x}, then \( P_1 \) sets \code{x} to \( 100 \), then \( P_2 \) sets \code{x}.
		\\
		\code{100}: &
		\( P_1 \) accesses \code{x} (twice), then \( P_2 \) sets \code{x} to \( 11 \), then \( P_1 \) sets \code{x}. \\
		\bottomrule
	\end{tabular}
\end{center}

We can constrain the concurrency by using serialized procedures, which are created by \newterm{serializers}.
Serializers are constructed by \code{make-serializer}, whose implementation is given below.
A serializer takes a procedure as argument and returns a serialized procedure that behaves like the original procedure.
All calls to a given serializer return serialized procedures in the same set.

Thus, in contrast to the example above, executing
\begin{scheme}
  (define x 10)

  (define s (make-serializer))

  (parallel-execute
   (s (lambda () (set! x (* x x))))
   (s (lambda () (set! x (+ x 1)))))
\end{scheme}
can produce only two possible values for \code{x}, \( 101 \) or \( 121 \).
The other possibilities are eliminated, because the execution of \( P_1 \) and \( P_2 \) cannot be interleaved.

Here is a version of the \code{make-account} procedure from \cref{Section 3.1.1}, where the deposits and withdrawals have been serialized:
\begin{scheme}
  (define (make-account balance)
    (define (withdraw amount)
      (if (>= balance amount)
          (begin (set! balance (- balance amount))
                 balance)
          "Insufficient funds"))
    (define (deposit amount)
      (set! balance (+ balance amount))
      balance)
    (let ((protected (make-serializer)))
      (define (dispatch m)
        (cond ((eq? m 'withdraw) (protected withdraw))
              ((eq? m 'deposit) (protected deposit))
              ((eq? m 'balance) balance)
              (else (error "Unknown request: MAKE-ACCOUNT"
                           m))))
      dispatch))
\end{scheme}
With this implementation, two processes cannot be withdrawing from or depositing into a single account concurrently.
This eliminates the source of the error illustrated in \cref{Figure 3.29}, where Peter changes the account balance between the times when Paul accesses the balance to compute the new value and when Paul actually performs the assignment.
On the other hand, each account has its own serializer, so that deposits and withdrawals for different accounts can proceed concurrently.



\begin{exercise}
	\label{Exercise 3.39}
	Which of the five possibilities in the parallel execution shown above remain if we instead serialize execution as follows:
	\begin{scheme}
	  (define x 10)
	  (define s (make-serializer))
	  (parallel-execute
	   (lambda () (set! x ((s (lambda () (* x x))))))
	   (s (lambda () (set! x (+ x 1)))))
	\end{scheme}
\end{exercise}



\begin{exercise}
	\label{Exercise 3.40}
	Give all possible values of \code{x} that can result from executing
	\begin{scheme}
	  (define x 10)
	  (parallel-execute (lambda () (set! x (* x x)))
	                    (lambda () (set! x (* x x x))))
	\end{scheme}
	Which of these possibilities remain if we instead use serialized procedures:
	\begin{scheme}
	  (define x 10)
	  (define s (make-serializer))
	  (parallel-execute (s (lambda () (set! x (* x x))))
	                    (s (lambda () (set! x (* x x x)))))
	\end{scheme}
\end{exercise}



\begin{exercise}
	\label{Exercise 3.41}
	Ben Bitdiddle worries that it would be better to implement the bank account as follows (where the commented line has been changed):
	\begin{scheme}
	  (define (make-account balance)
	    (define (withdraw amount)
	      (if (>= balance amount)
	          (begin (set! balance
	                       (- balance amount))
	                 balance)
	          "Insufficient funds"))
	    (define (deposit amount)
	      (set! balance (+ balance amount))
	      balance)
	    (let ((protected (make-serializer)))
	      (define (dispatch m)
	        (cond ((eq? m 'withdraw) (protected withdraw))
	              ((eq? m 'deposit) (protected deposit))
	              ((eq? m 'balance)
	               ((protected
	                 (lambda () balance)))) ~\textrm{; serialized}~
	              (else
	               (error "Unknown request: MAKE-ACCOUNT"
	                      m))))
	      dispatch))
	\end{scheme}
	because allowing unserialized access to the bank balance can result in anomalous behavior.
	Do you agree?
	Is there any scenario that demonstrates Ben’s concern?
\end{exercise}



\begin{exercise}
	\label{Exercise 3.42}
	Ben Bitdiddle suggests that it’s a waste of time to create a new serialized procedure in response to every \code{withdraw} and \code{deposit} message.
	He says that \code{make-account} could be changed so that the calls to \code{protected} are done outside the \code{dispatch} procedure.
	That is, an account would return the same serialized procedure (which was created at the same time as the account) each time it is asked for a withdrawal procedure.
	\begin{scheme}
	  (define (make-account balance)
	    (define (withdraw amount)
	      (if (>= balance amount)
	          (begin (set! balance (- balance amount))
	                 balance)
	          "Insufficient funds"))
	    (define (deposit amount)
	      (set! balance (+ balance amount))
	      balance)
	    (let ((protected (make-serializer)))
	      (let ((protected-withdraw (protected withdraw))
	            (protected-deposit (protected deposit)))
	        (define (dispatch m)
	          (cond ((eq? m 'withdraw) protected-withdraw)
	                ((eq? m 'deposit) protected-deposit)
	                ((eq? m 'balance) balance)
	                (else
	                 (error "Unknown request: MAKE-ACCOUNT"
	                        m))))
	        dispatch)))
	\end{scheme}
	Is this a safe change to make?
	In particular, is there any difference in what concurrency is allowed by these two versions of \code{make-account}?
\end{exercise}



\subsubsection*{Complexity of using multiple shared resources}

Serializers provide a powerful abstraction that helps isolate the complexities of concurrent programs so that they can be dealt with carefully and (hopefully) correctly.
However, while using serializers is relatively straightforward when there is only a single shared resource (such as a single bank account), concurrent programming can be treacherously difficult when there are multiple shared resources.

To illustrate one of the difficulties that can arise, suppose we wish to swap the balances in two bank accounts.
We access each account to find the balance, compute the difference between the balances, withdraw this difference from one account, and deposit it in the other account.
We could implement this as follows:%
\footnote{
	We have simplified \code{exchange} by exploiting the fact that our \code{deposit} message accepts negative amounts.
	(This is a serious bug in our banking system!)
}
\begin{scheme}
  (define (exchange account1 account2)
    (let ((difference (- (account1 'balance)
                         (account2 'balance))))
      ((account1 'withdraw) difference)
      ((account2 'deposit) difference)))
\end{scheme}

This procedure works well when only a single process is trying to do the exchange.
Suppose, however, that Peter and Paul both have access to accounts \( a \)1, \( a \)2, and \( a \)3, and that Peter exchanges \( a \)1 and \( a \)2 while Paul concurrently exchanges \( a \)1 and \( a \)3.
Even with account deposits and withdrawals serialized for individual accounts (as in the \code{make-account} procedure shown above in this section), \code{exchange} can still produce incorrect results.
For example, Peter might compute the difference in the balances for \( a \)1 and \( a \)2, but then Paul might change the balance in \( a \)1 before Peter is able to complete the exchange.%
\footnote{
	If the account balances start out as \dollar{10}, \dollar{20}, and \dollar{30}, then after any number of concurrent exchanges, the balances should still be \dollar{10}, \dollar{20}, and \dollar{30} in some order.
	Serializing the deposits to individual accounts is not sufficient to guarantee this.
	See \cref{Exercise 3.43}.
}
For correct behavior, we must arrange for the \code{exchange} procedure to lock out any other concurrent accesses to the accounts during the entire time of the exchange.

One way we can accomplish this is by using both accounts’ serializers to serialize the entire \code{exchange} procedure.
To do this, we will arrange for access to an account’s serializer.
Note that we are deliberately breaking the modularity of the bank-account object by exposing the serializer.
The following version of \code{make-account} is identical to the original version given in \cref{Section 3.1.1}, except that a serializer is provided to protect the balance variable, and the serializer is exported via message passing:
\begin{scheme}
  (define (make-account-and-serializer balance)
    (define (withdraw amount)
      (if (>= balance amount)
          (begin (set! balance (- balance amount))
                 balance)
          "Insufficient funds"))
    (define (deposit amount)
      (set! balance (+ balance amount))
      balance)
    (let ((balance-serializer (make-serializer)))
      (define (dispatch m)
        (cond ((eq? m 'withdraw) withdraw)
              ((eq? m 'deposit) deposit)
              ((eq? m 'balance) balance)
              ((eq? m 'serializer) balance-serializer)
              (else (error "Unknown request: MAKE-ACCOUNT" m))))
      dispatch))
\end{scheme}

We can use this to do serialized deposits and withdrawals.
However, unlike our earlier serialized account, it is now the responsibility of each user of bank-account objects to explicitly manage the serialization, for example as follows:%
\footnote{
	\cref{Exercise 3.45} investigates why deposits and withdrawals are no longer automatically serialized by the account.
}
\begin{scheme}
  (define (deposit account amount)
    (let ((s (account 'serializer))
          (d (account 'deposit)))
      ((s d) amount)))
\end{scheme}

Exporting the serializer in this way gives us enough flexibility to implement a serialized exchange program.
We simply serialize the original \code{exchange} procedure with the serializers for both accounts:
\begin{scheme}
  (define (serialized-exchange account1 account2)
    (let ((serializer1 (account1 'serializer))
          (serializer2 (account2 'serializer)))
      ((serializer1 (serializer2 exchange))
       account1
       account2)))
\end{scheme}



\begin{exercise}
	\label{Exercise 3.43}
	Suppose that the balances in three accounts start out as \dollar{10}, \dollar{20}, and \dollar{30}, and that multiple processes run, exchanging the balances in the accounts.
	Argue that if the processes are run sequentially, after any number of concurrent exchanges, the account balances should be \dollar{10}, \dollar{20}, and \dollar{30} in some order.
	Draw a timing diagram like the one in \cref{Figure 3.29} to show how this condition can be violated if the exchanges are implemented using the first version of the account-exchange program in this section.
	On the other hand, argue that even with this \code{exchange} program, the sum of the balances in the accounts will be preserved.
	Draw a timing diagram to show how even this condition would be violated if we did not serialize the transactions on individual accounts.
\end{exercise}



\begin{exercise}
	\label{Exercise 3.44}
	Consider the problem of transferring an amount from one account to another.
	Ben Bitdiddle claims that this can be accomplished with the following procedure, even if there are multiple people concurrently transferring money among multiple accounts, using any account mechanism that serializes deposit and withdrawal transactions, for example, the version of \code{make-account} in the text above.
	\begin{scheme}
	  (define (transfer from-account to-account amount)
	    ((from-account 'withdraw) amount)
	    ((to-account 'deposit) amount))
	\end{scheme}
	Louis Reasoner claims that there is a problem here, and that we need to use a more sophisticated method, such as the one required for dealing with the exchange problem.
	Is Louis right?
	If not, what is the essential difference between the transfer problem and the exchange problem?
	(You should assume that the balance in \code{from-account} is at least \code{amount}.)
\end{exercise}



\begin{exercise}
	\label{Exercise 3.45}
	Louis Reasoner thinks our bank-account system is unnecessarily complex and error-prone now that deposits and withdrawals aren’t automatically serialized.
	He suggests that \code{make-account-and-serializer} should have exported the serializer (for use by such procedures as \code{serialized-exchange}) in addition to (rather than instead of) using it to serialize accounts and deposits as \code{make-account} did.
	He proposes to redefine accounts as follows:
	\begin{smallscheme}
	  (define (make-account-and-serializer balance)
	    (define (withdraw amount)
	      (if (>= balance amount)
	          (begin (set! balance (- balance amount)) balance)
	          "Insufficient funds"))
	    (define (deposit amount)
	      (set! balance (+ balance amount)) balance)
	    (let ((balance-serializer (make-serializer)))
	      (define (dispatch m)
	        (cond ((eq? m 'withdraw) (balance-serializer withdraw))
	              ((eq? m 'deposit) (balance-serializer deposit))
	              ((eq? m 'balance) balance)
	              ((eq? m 'serializer) balance-serializer)
	              (else (error "Unknown request: MAKE-ACCOUNT" m))))
	      dispatch))
	\end{smallscheme}
	Then deposits are handled as with the original \code{make-account}:
	\begin{scheme}
	  (define (deposit account amount)
	    ((account 'deposit) amount))
	\end{scheme}
	Explain what is wrong with Louis’s reasoning.
	In particular, consider what happens when \code{serialized-exchange} is called.
\end{exercise}



\subsubsection*{Implementing serializers}

We implement serializers in terms of a more primitive synchronization mechanism called a \newterm{mutex}.
A mutex is an object that supports two operations---the mutex can be \newterm{acquired}, and the mutex can be \newterm{released}.
Once a mutex has been acquired, no other acquire operations on that mutex may proceed until the mutex is released.%
\footnote{
	The term “mutex” is an abbreviation for \newterm{mutual exclusion}.
	The general problem of arranging a mechanism that permits concurrent processes to safely share resources is called the mutual exclusion problem.
	Our mutex is a simple variant of the \newterm{semaphore} mechanism (see \cref{Exercise 3.47}), which was introduced in the “THE” Multiprogramming System developed at the Technological University of Eindhoven and named for the university’s initials in Dutch (\cref{Dijkstra 1968a}).
	The acquire and release operations were originally called P and V, from the Dutch words \emph{passeren} (to pass) and \emph{vrijgeven} (to release), in reference to the semaphores used on railroad systems.
	Dijkstra’s classic exposition (\cref{Dijkstra 1968b}) was one of the first to clearly present the issues of concurrency control, and showed how to use semaphores to handle a variety of concurrency problems.
}
In our implementation, each serializer has an associated mutex.
Given a procedure \code{p}, the serializer returns a procedure that acquires the mutex, runs \code{p}, and then releases the mutex.
This ensures that only one of the procedures produced by the serializer can be running at once, which is precisely the serialization property that we need to guarantee.
\begin{scheme}
  (define (make-serializer)
    (let ((mutex (make-mutex)))
      (lambda (p)
        (define (serialized-p . args)
          (mutex 'acquire)
          (let ((val (apply p args)))
            (mutex 'release)
            val))
        serialized-p)))
\end{scheme}

The mutex is a mutable object (here we’ll use a one-element list, which we’ll refer to as a \newterm{cell}) that can hold the value true or false.
When the value is false, the mutex is available to be acquired.
When the value is true, the mutex is unavailable, and any process that attempts to acquire the mutex must wait.

Our mutex constructor \code{make-mutex} begins by initializing the cell contents to false.
To acquire the mutex, we test the cell.
If the mutex is available, we set the cell contents to true and proceed.
Otherwise, we wait in a loop, attempting to acquire over and over again, until we find that the mutex is available.%
\footnote{
	In most time-shared operating systems, processes that are blocked by a mutex do not waste time “busy-waiting” as above.
	Instead, the system schedules another process to run while the first is waiting, and the blocked process is awakened when the mutex becomes available.
}
To release the mutex, we set the cell contents to false.
\begin{scheme}
  (define (make-mutex)
    (let ((cell (list false)))
      (define (the-mutex m)
        (cond ((eq? m 'acquire)
               (if (test-and-set! cell)
                   (the-mutex 'acquire))) ~\textrm{; retry}~
              ((eq? m 'release) (clear! cell))))
      the-mutex))
  (define (clear! cell) (set-car! cell false))
\end{scheme}

\code{test-and-set!} tests the cell and returns the result of the test.
In addition, if the test was false, \code{test-and-set!} sets the cell contents to true before returning false.
We can express this behavior as the following procedure:
\begin{scheme}
  (define (test-and-set! cell)
    (if (car cell) true (begin (set-car! cell true) false)))
\end{scheme}

However, this implementation of \code{test-and-set!} does not suffice as it stands.
There is a crucial subtlety here, which is the essential place where concurrency control enters the system:
The \code{test-and-set!} operation must be performed \newterm{atomically}.
That is, we must guarantee that, once a process has tested the cell and found it to be false, the cell contents will actually be set to true before any other process can test the cell.
If we do not make this guarantee, then the mutex can fail in a way similar to the bank-account failure in \cref{Figure 3.29}.
(See \cref{Exercise 3.46}.)

The actual implementation of \code{test-and-set!} depends on the details of how our system runs concurrent processes.
For example, we might be executing concurrent processes on a sequential processor using a time-slicing mechanism that cycles through the processes, permitting each process to run for a short time before interrupting it and moving on to the next process.
In that case, \code{test-and-set!} can work by disabling time slicing during the testing and setting.%
\footnote{
	In \acronym{MIT} Scheme for a single processor, which uses a
	time-slicing model, \code{test-and-set!} can be implemented as follows:
	\begin{smallscheme}
	  (define (test-and-set! cell)
	    (without-interrupts
	     (lambda ()
	       (if (car cell)
	           true
	           (begin (set-car! cell true)
	                  false)))))
	\end{smallscheme}
	\code{without-interrupts} disables time-slicing interrupts while its procedure
	argument is being executed.
}
Alternatively, multiprocessing computers provide instructions that support atomic operations directly in hardware.%
\footnote{
	There are many variants of such instructions---including test-and-set, test-and-clear, swap, compare-and-exchange, load-reserve, and store-conditional---whose design must be carefully matched to the machine’s processor-memory interface.
	One issue that arises here is to determine what happens if two processes attempt to acquire the same resource at exactly the same time by using such an instruction.
	This requires some mechanism for making a decision about which process gets control.
	Such a mechanism is called an \newterm{arbiter}.
	Arbiters usually boil down to some sort of hardware device.
	Unfortunately, it is possible to prove that one cannot physically construct a fair arbiter that works 100\% of the time unless one allows the arbiter an arbitrarily long time to make its decision.
	The fundamental phenomenon here was originally observed by the fourteenth-century French philosopher Jean Buridan in his commentary on Aristotle’s \textit{De caelo}.
	Buridan argued that a perfectly rational dog placed between two equally attractive sources of food will starve to death, because it is incapable of deciding which to go to first.
}



\begin{exercise}
	\label{Exercise 3.46}
	Suppose that we implement \code{test-and-set!} using an ordinary procedure as shown in the text, without attempting to make the operation atomic.
	Draw a timing diagram like the one in \cref{Figure 3.29} to demonstrate how the mutex implementation can fail by allowing two processes to acquire the mutex at the same time.
\end{exercise}



\begin{exercise}
	\label{Exercise 3.47}
	A semaphore (of size \( n \)) is a generalization of a mutex.
	Like a mutex, a semaphore supports acquire and release operations, but it is more general in that up to \( n \) processes can acquire it concurrently.
	Additional processes that attempt to acquire the semaphore must wait for release operations.
	Give implementations of semaphores
	\begin{enumerate}[label = \alph*., leftmargin = *]

		\item
			in terms of mutexes

		\item
			in terms of atomic \code{test-and-set!} operations.

	\end{enumerate}
\end{exercise}



\subsubsection*{Deadlock}

Now that we have seen how to implement serializers, we can see that account exchanging still has a problem, even with the \code{serialized-exchange} procedure above.
Imagine that Peter attempts to exchange \( a \)1 with \( a \)2 while Paul concurrently attempts to exchange \( a \)2 with \( a \)1.
Suppose that Peter’s process reaches the point where it has entered a serialized procedure protecting \( a \)1 and, just after that, Paul’s process enters a serialized procedure protecting \( a \)2.
Now Peter cannot proceed (to enter a serialized procedure protecting \( a \)2) until Paul exits the serialized procedure protecting \( a \)2.
Similarly, Paul cannot proceed until Peter exits the serialized procedure protecting \( a \)1.
Each process is stalled forever, waiting for the other.
This situation is called a \newterm{deadlock}.
Deadlock is always a danger in systems that provide concurrent access to multiple shared resources.

One way to avoid the deadlock in this situation is to give each account a unique identification number and rewrite \code{serialized-exchange} so that a process will always attempt to enter a procedure protecting the lowest-numbered account first.
Although this method works well for the exchange problem, there are other situations that require more sophisticated deadlock-avoidance techniques, or where deadlock cannot be avoided at all.
(See \cref{Exercise 3.48} and \cref{Exercise 3.49}.)%
\footnote{
	The general technique for avoiding deadlock by numbering the shared resources and acquiring them in order is due to \cref{Havender (1968)}.
	Situations where deadlock cannot be avoided require \newterm{deadlock-recovery} methods, which entail having processes “back out” of the deadlocked state and try again.
	Deadlock-recovery mechanisms are widely used in database management systems, a topic that is treated in detail in \cref{Gray and Reuter 1993}.
}



\begin{exercise}
	\label{Exercise 3.48}
	Explain in detail why the deadlock-avoidance method described above, (i.e., the accounts are numbered, and each process attempts to acquire the smaller-numbered account first) avoids deadlock in the exchange problem.
	Re\-write \code{serialized-exchange} to incorporate this idea.
	(You will also need to modify \code{make-account} so that each account is created with a number, which can be accessed by sending an appropriate message.)
\end{exercise}



\begin{exercise}
	\label{Exercise 3.49}
	Give a scenario where the deadlock-avoid\-ance mechanism described above does not work.
	(Hint:
	In the exchange problem, each process knows in advance which accounts it will need to get access to.
	Consider a situation where a process must get access to some shared resources before it can know which additional shared resources it will require.)
\end{exercise}



\subsubsection*{Concurrency, time, and communication}

We’ve seen how programming concurrent systems requires controlling the ordering of events when different processes access shared state, and we’ve seen how to achieve this control through judicious use of serializers.
But the problems of concurrency lie deeper than this, because, from a fundamental point of view, it’s not always clear what is meant by “shared state.”

Mechanisms such as \code{test-and-set!} require processes to examine a global shared flag at arbitrary times.
This is problematic and inefficient to implement in modern high-speed processors, where due to optimization techniques such as pipelining and cached memory, the contents of memory may not be in a consistent state at every instant.
In contemporary multiprocessing systems, therefore, the serializer paradigm is being supplanted by new approaches to concurrency control.%
\footnote{
	One such alternative to serialization is called \newterm{barrier synchronization}.
	The programmer permits concurrent processes to execute as they please, but establishes certain synchronization points (“barriers”) through which no process can proceed until all the processes have reached the barrier.
	Modern processors provide machine instructions that permit programmers to establish synchronization points at places where consistency is required.
	The PowerPC, for example, includes for this purpose two instructions called \acronym{SYNC} and \acronym{EIEIO} (Enforced In-order Execution of Input/Output).
}

The problematic aspects of shared state also arise in large, distributed systems.
For instance, imagine a distributed banking system where individual branch banks maintain local values for bank balances and periodically compare these with values maintained by other branches.
In such a system the value of “the account balance” would be undetermined, except right after synchronization.
If Peter deposits money in an account he holds jointly with Paul, when should we say that the account balance has changed---when the balance in the local branch changes, or not until after the synchronization?
And if Paul accesses the account from a different branch, what are the reasonable constraints to place on the banking system such that the behavior is “correct”?
The only thing that might matter for correctness is the behavior observed by Peter and Paul individually and the “state” of the account immediately after synchronization.
Questions about the “real” account balance or the order of events between synchronizations may be irrelevant or meaningless.%
\footnote{
	This may seem like a strange point of view, but there are systems that work this way.
	International charges to credit-card accounts, for example, are normally cleared on a per-country basis, and the charges made in different countries are periodically reconciled.
	Thus the account balance may be different in different countries.
}

The basic phenomenon here is that synchronizing different processes, establishing shared state, or imposing an order on events requires communication among the processes.
In essence, any notion of time in concurrency control must be intimately tied to communication.%
\footnote{
	For distributed systems, this perspective was pursued by \cref{Lamport (1978)}, who showed how to use communication to establish “global clocks” that can be used to establish orderings on events in distributed systems.
}
It is intriguing that a similar connection between time and communication also arises in the Theory of Relativity, where the speed of light (the fastest signal that can be used to synchronize events) is a fundamental constant relating time and space.
The complexities we encounter in dealing with time and state in our computational models may in fact mirror a fundamental complexity of the physical universe.
